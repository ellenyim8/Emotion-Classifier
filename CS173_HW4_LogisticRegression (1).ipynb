{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2"
      ],
      "metadata": {
        "id": "kY9dgdkxb7xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2.1"
      ],
      "metadata": {
        "id": "3csYOOFmb-9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgaILxNuaKG5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.sentiment.util import split_train_test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('hw4LR-CS173-published-sheet.csv')\n",
        "#print(len(df))\n",
        "df = df.replace({r'\\n': ' '}, regex=True)\n",
        "\n",
        "training_set = df[0:30]\n",
        "#print(len(training_set))\n",
        "\n",
        "validation_set = df[30:40]\n",
        "#print(len(validation_set))\n",
        "\n",
        "testing_set = df[40:]\n",
        "df = df.replace({r'\\n': ' '}, regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore #\n",
        "#nltk.download('punkt')\n",
        "#from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "#stops = set(stopwords.words('english'))\n",
        "#print(stops)"
      ],
      "metadata": {
        "id": "W10mNTLQht65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2.2"
      ],
      "metadata": {
        "id": "emBJ80WkcCPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_columns = ['Fear Lexicons', 'Fear Sentences', 'Anger Lexicons', 'Anger Sentences', 'Surprise Lexicons', 'Surprise Sentence', 'Disgust Lexicons', 'Disgust Sentences', 'Fear + Anger Lexicons', 'Fear + Anger Sentences', 'Surprise + Disgust Lexicons', 'Surprise + Disgust Sentences', 'Sadness + Joy Lexicons', 'Sadness + Joy Sentences', 'Sadness + Joy + Fear Lexicons', 'Sadness + Joy + Fear Sentences']\n",
        "df = df.drop(drop_columns, axis=1).dropna()\n",
        "\n",
        "sadlexicons = training_set['Sadness Lexicons']\n",
        "joylexicons = training_set['Joy Lexicons']\n",
        "sadsentences = training_set['Sadness Sentences']\n",
        "joysentences = training_set['Joy Sentences']\n",
        "\n",
        "sadwords = list()\n",
        "joywords = list()\n",
        "\n",
        "for r in joylexicons:\n",
        "  r = r.split(',')\n",
        "\n",
        "  #print(r)\n",
        "  for word in r:\n",
        "    word = word.lstrip()\n",
        "    word = word.rstrip()\n",
        "\n",
        "    if word not in joywords:\n",
        "      joywords.append(word)\n",
        "\n",
        "print(joywords)\n",
        "print('the number of joy lexicons: ', len(joywords))\n",
        "\n",
        "for r in sadlexicons:\n",
        "  r = r.split(',')\n",
        "\n",
        "  for word in r:\n",
        "    word = word.lstrip()\n",
        "    word = word.rstrip()\n",
        "    if word not in sadwords:\n",
        "      sadwords.append(word)\n",
        "\n",
        "print(sadwords)\n",
        "print('the number of sad lexicons: ', len(sadwords))\n",
        "\n",
        "classes = {'Sadness', 'Joy'}\n",
        "\n",
        "sadlexicons_count = dict()\n",
        "joylexicons_count = dict()\n",
        "\n",
        "words_total_document = 0\n"
      ],
      "metadata": {
        "id": "dofjk3H5cDpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2a8ef4-5eff-4849-913f-56cc05c3a30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['child', 'laughter', 'sunny', 'youth', 'zeal', 'Fidelity', 'Praiseworthy', 'absolution', 'abundance', 'abundant', 'accolade', 'accompaniment', 'accomplish', 'accomplished', 'achieve', 'achievement', 'acrobat', 'admirable', 'admiration', 'adorable', 'adoration', 'adore', 'advance', 'advent', 'advocacy', 'aesthetics', 'affection', 'affluence', 'alive', 'allure', 'aloha', 'amazingly', 'ambition', 'amen', 'amicable', 'amnesty', 'amour', 'amuse', 'amused', 'amusement', 'amusing', 'angel', 'angelic', 'animated', 'applause', 'appreciation', 'approve', 'ardent', 'art', 'aspiration', 'aspire', 'aspiring', 'astonishment', 'atone', 'auspicious', 'authentic', 'award', '', 'engaged', 'rekindle', 'favoriable', 'feeling', 'happy', 'Delighted', 'serenity', 'unconstrained', 'Heartily', 'complement', 'Amusing', 'Charity', 'Abundance', 'Aspire', 'Love', 'Generous', 'Merry', 'chuckle', 'perfection', 'festive', 'succeed', 'enthusiastic', 'laugh', 'excitement', 'jump', 'hero', 'ditty', 'playhouse', 'rejoicing', 'entertainment', 'daughter', 'Felicity', 'Smiling', 'Glee', 'treasure', 'Gratifying', 'joy', 'laughing', 'happiness', 'healing', 'utopian', 'perfect', 'smile', 'Young', 'zealous', 'winner', 'Alive', 'Beautiful', 'sweet', 'glee', 'Zealous', 'Exhilaration', 'exuberance', 'delicious', 'beach', 'love', 'Miracle', 'delighted', 'favorite', 'friendship', 'blossom\\r (blossomed)', 'fulfillment', 'wonderful', 'twinkle', 'birthday', 'happily', 'beautiful']\n",
            "the number of joy lexicons:  125\n",
            "['abduction', 'devastate', 'wretched', 'wrongly', 'Stifle', 'Mausoleum', 'abandon', 'abandoned', 'abandonment', 'abortion', 'abortive', 'abscess', 'absence', 'absent', 'absentee', 'abuse', 'abysmal', 'abyss', 'accident', 'accursed', 'ache', 'aching', 'adder', 'adrift', 'adultery', 'adverse', 'adversity', 'afflict', 'affliction', 'affront', 'aftermath', 'aggravating', 'agony', 'ail', 'ailing', 'alcoholism', 'alienated', 'alienation', 'anathema', 'anchorage', 'anguish', 'animosity', 'annihilated', 'annihilation', 'annulment', 'anthrax', 'antisocial', 'anxiety', 'apathetic', 'apathy', 'apologize', 'appendicitis', 'arid', 'arraignment', 'arsenic', '', 'lonely', 'wrongful', 'Blunder', 'crushed', 'tragedy', 'Lonely', 'longing', 'Robbery', 'Dying', 'Hospital', 'Crippled', 'Horror', 'Sufferer', 'homesick', 'remorse', 'lowest', 'devastating', 'demise', 'Agony', 'savage', 'betrayed', 'stripped', 'desert', 'undesirable', 'deceased', 'desolation', 'weeping', 'Disconnect', 'Damages', 'Unsuccessful', 'Weeping', 'Interrupted', 'Fatal', 'struggle', 'overwhelmed', 'worthlessness', 'isolate', 'grief', 'Abandon', 'death', 'abduct', 'Anguish', 'endless', 'Abandoned', 'Despair', 'unrequited', 'Forgotten', 'suffering', 'degrading', 'deformity', 'lone', 'isolation', 'inability']\n",
            "the number of sad lexicons:  109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = []   # whole document (Sadness sentences + joy sentences)\n",
        "features = []  # features (vectors) for each sentence (first col=count of joy lexicon in document, 2nd col=count of sad lexicon in document, 3rd col=total # of tokens in sentence)\n",
        "\n",
        "nltk.download('punkt')\n",
        "for sent in joysentences:\n",
        "  feature = []\n",
        "  x1 = 0\n",
        "  x2 = 0\n",
        "  x3 = 0\n",
        "\n",
        "  sent = sent_tokenize(sent)\n",
        "  #print(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "\n",
        "    for w in words:\n",
        "      if w in joywords:\n",
        "        if w not in joylexicons_count:\n",
        "          joylexicons_count[w] = 1\n",
        "        else:\n",
        "          joylexicons_count[w] += 1\n",
        "        x1 = joylexicons_count[w]\n",
        "\n",
        "      if w in sadwords:\n",
        "        if w not in sadlexicons_count:\n",
        "          sadlexicons_count[w] = 1\n",
        "        else:\n",
        "          sadlexicons_count[w] += 1\n",
        "        x2 = sadlexicons_count[w]\n",
        "      #joywords_doc.append(w)\n",
        "    words_total_document = len(words)\n",
        "    x3 = words_total_document\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  features.append(feature)\n",
        "  document.append(sent)\n",
        "\n",
        "print(\"Joy lexicons count in joy sentences \")\n",
        "print(joylexicons_count)\n",
        "\n",
        "for sent in sadsentences:\n",
        "  feature = []\n",
        "  x1 = 0\n",
        "  x2 = 0\n",
        "  x3 = 0\n",
        "\n",
        "  sent = sent_tokenize(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "\n",
        "    for w in words:\n",
        "      if w in sadwords:\n",
        "        if w not in sadlexicons_count:\n",
        "         sadlexicons_count[w] = 1\n",
        "        else:\n",
        "          sadlexicons_count[w] += 1\n",
        "        x2 = sadlexicons_count[w]\n",
        "\n",
        "      if w in joywords:\n",
        "        if w not in joylexicons_count:\n",
        "          joylexicons_count[w] = 1\n",
        "        else:\n",
        "          joylexicons_count[w] += 1\n",
        "        x1 = joylexicons_count[w]\n",
        "\n",
        "      #sadwords_doc.append(w)\n",
        "    words_total_document = len(words)\n",
        "    x3 = words_total_document\n",
        "    #print(words_total_document)\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  features.append(feature)\n",
        "  document.append(sent)\n",
        "\n",
        "print(\"Sad lexicons count in sad sentences \")\n",
        "print(sadlexicons_count)\n",
        "\n",
        "#x1 = len(joylexicons_count) # Counts of joy lexicon in the document (sentences)\n",
        "#x2 = len(sadlexicons_count)  # counts of sad lexicons in document (sentences )\n",
        "#x3 = words_total_document\n",
        "\n",
        "for f in features:\n",
        "  print(f)\n",
        "# x1 = count(\"awesome\") in document\n",
        "# x2 = count of sad lexicons in document\n",
        "# x3 = total number of tokens"
      ],
      "metadata": {
        "id": "ziJ-2wudyv6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765bdd2c-5b07-4d31-d85d-ddba2b0e9933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joy lexicons count in joy sentences \n",
            "{'sunny': 1, 'laughter': 1, 'youth': 1, 'zeal': 1, 'adorable': 1, 'rekindle': 1, 'friendship': 2, 'engaged': 1, 'child': 1, 'happy': 2, 'achieve': 1, 'delighted': 2, 'unconstrained': 1, 'serenity': 1, 'love': 4, 'amusing': 2, 'abundance': 1, 'aspire': 1, 'perfection': 2, 'festive': 1, 'chuckle': 1, 'joy': 4, 'excitement': 2, 'hero': 1, 'daughter': 1, 'glee': 2, 'beach': 2, 'treasure': 1, 'beautiful': 3, 'Gratifying': 1, 'happiness': 1, 'healing': 1, 'achievement': 1, 'utopian': 1, 'smile': 1, 'zealous': 2, 'fulfillment': 2, 'winner': 1, 'accompaniment': 1, 'alive': 1, 'sweet': 1, 'perfect': 1, 'exuberance': 1, 'delicious': 1, 'award': 1, 'amour': 1, 'art': 1, 'favorite': 1, 'birthday': 1, 'twinkle': 1, 'wonderful': 1, 'happily': 1}\n",
            "Sad lexicons count in sad sentences \n",
            "{'devastating': 2, 'abduction': 1, 'wretched': 1, 'wrongly': 1, 'lonely': 2, 'abandoned': 4, 'death': 2, 'agony': 3, 'crushed': 1, 'tragedy': 1, 'longing': 3, 'suffering': 2, 'remorse': 1, 'homesick': 1, 'lowest': 1, 'demise': 1, 'savage': 1, 'betrayed': 1, 'ache': 1, 'abysmal': 1, 'deceased': 1, 'weeping': 1, 'grief': 1, 'abuse': 2, 'abandon': 1, 'anguish': 2, 'endless': 1, 'abyss': 1, 'unrequited': 1, 'alcoholism': 1, 'accident': 1, 'anxiety': 1, 'degrading': 1, 'deformity': 1, 'lone': 1, 'isolation': 1, 'inability': 1}\n",
            "[1, 0, 25]\n",
            "[1, 0, 7]\n",
            "[0, 0, 10]\n",
            "[1, 0, 10]\n",
            "[1, 0, 22]\n",
            "[1, 0, 18]\n",
            "[1, 0, 22]\n",
            "[1, 0, 21]\n",
            "[0, 0, 21]\n",
            "[1, 0, 18]\n",
            "[1, 0, 27]\n",
            "[0, 0, 41]\n",
            "[2, 0, 23]\n",
            "[1, 0, 20]\n",
            "[1, 0, 11]\n",
            "[2, 0, 16]\n",
            "[1, 0, 31]\n",
            "[1, 0, 13]\n",
            "[1, 0, 11]\n",
            "[1, 0, 37]\n",
            "[1, 0, 9]\n",
            "[1, 0, 23]\n",
            "[1, 0, 11]\n",
            "[1, 0, 27]\n",
            "[4, 0, 31]\n",
            "[2, 0, 27]\n",
            "[1, 0, 27]\n",
            "[0, 0, 20]\n",
            "[1, 0, 24]\n",
            "[1, 0, 15]\n",
            "[2, 1, 20]\n",
            "[0, 1, 7]\n",
            "[0, 0, 12]\n",
            "[1, 0, 20]\n",
            "[0, 1, 21]\n",
            "[3, 1, 14]\n",
            "[0, 1, 16]\n",
            "[0, 1, 16]\n",
            "[2, 1, 22]\n",
            "[0, 0, 28]\n",
            "[0, 1, 10]\n",
            "[0, 1, 24]\n",
            "[0, 1, 23]\n",
            "[0, 3, 14]\n",
            "[0, 1, 8]\n",
            "[0, 0, 18]\n",
            "[0, 1, 13]\n",
            "[0, 0, 8]\n",
            "[0, 1, 21]\n",
            "[0, 2, 45]\n",
            "[0, 2, 18]\n",
            "[0, 1, 26]\n",
            "[0, 1, 7]\n",
            "[0, 1, 21]\n",
            "[0, 4, 29]\n",
            "[6, 1, 36]\n",
            "[0, 2, 24]\n",
            "[0, 0, 32]\n",
            "[0, 1, 14]\n",
            "[0, 1, 17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3\n",
        "#### Logistic Regression classifier"
      ],
      "metadata": {
        "id": "g5cPne7_1Bqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 3.1"
      ],
      "metadata": {
        "id": "4Y3iWWlr1Lwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "#import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from math import exp"
      ],
      "metadata": {
        "id": "ZXPAjbZN7ERu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(row, coeff):\n",
        "  yhat = coeff[0]\n",
        "  for i in range(len(row) - 1):\n",
        "    yhat += coeff[i-1] * row[i]\n",
        "\n",
        "  return 1.0 / (1.0 + exp(-yhat))"
      ],
      "metadata": {
        "id": "XM9_IumGA4Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing 3.1 stuff\n",
        "classes_joy_sad_dataset = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "i = 0\n",
        "for row in features:\n",
        "  row.append(classes_joy_sad_dataset[i])\n",
        "  i += 1\n",
        "  #print(row)\n",
        "\n",
        "coef = [0, 0, 1]\n",
        "\n",
        "for row in features:\n",
        "  yhat = prediction(row, coef)\n",
        "  #print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))\n",
        "\n",
        "# yhat = 1.0 / (1.0 + e^(-(b0 + b1 * x1)))\n",
        "# yhat = predicted output\n",
        "# b0 = bias / intercept term\n",
        "# b1 = coeff for single input value (x1)\n",
        "# b = b + learning_rate * (y - yhat) * yhat * (1 - yhat) * x\n",
        "# b = coeff/weight being optimized\n",
        "# learning_rate = rate must configure\n",
        "# y - yhat = prediction error for model on training data attributed to weight\n",
        "# yhat = prediction made by coeff and x is input value\n",
        "# referenced: https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/"
      ],
      "metadata": {
        "id": "mhWqRUR7gvFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IGNORE CELL BELOW"
      ],
      "metadata": {
        "id": "t4p1rc_7HrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# p(y=1)  ; y = document, 1 = joy class\n",
        "# z = w*x + b\n",
        "# compute w*x + b\n",
        "#lb = LabelEncoder()\n",
        "#y_train = lb.fit_transform(y_train)\n",
        "#y_test = lb.transform(y_test)\n",
        "\n",
        "#plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "\n",
        "#joy = document[:30]\n",
        "#sad = document[30:]\n",
        "\n",
        "#def sigmoid(w, x, b):\n",
        "#  return w*x + b\n",
        "#def sigmoidFunction(z):\n",
        "#  return 1 / (1 + exp(-z))\n",
        "\n",
        "#logreg_training_set = training_set.drop(drop_columns, axis=1)\n",
        "#logreg_training_set\n",
        "\n",
        "# training_set.info()"
      ],
      "metadata": {
        "id": "B3WGYdHN1Agh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 3.2"
      ],
      "metadata": {
        "id": "z-BNiYCb1Ory"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import log\n",
        "weights = [0,0,0]\n",
        "# loss function on first sentence\n",
        "# Lce (yhat, y) = -[y log sigmoid (w*x + b) + (1-y)log(1-sigmoid(w*x+b))]\n",
        "def Lce(y):\n",
        "  return -log(y)\n",
        "\n",
        "row = features[0]\n",
        "print(row)\n",
        "pred = prediction(row, weights)\n",
        "print(pred)\n",
        "loss = Lce(pred)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "SywiNXiD1Qbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ac87dd-3b99-474d-f7a3-8a4f2cdf50f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 25, 1]\n",
            "0.5\n",
            "0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4"
      ],
      "metadata": {
        "id": "ZpCfEES98_SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 4.1"
      ],
      "metadata": {
        "id": "LD0z9-JC9BXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
        "#validation_set\n",
        "\n",
        "df1 = validation_set.drop(drop_columns, axis=1) # dropping all columns except Sadness and Joy (lexicons and sentences) for validation set\n",
        "\n",
        "vs_sadlexicons = validation_set['Sadness Lexicons']\n",
        "vs_joylexicons = validation_set['Joy Lexicons']\n",
        "vs_sadsentences = validation_set['Sadness Sentences']\n",
        "vs_joysentences = validation_set['Joy Sentences']\n",
        "\n",
        "vs_sadwords = list()\n",
        "vs_joywords = list()\n",
        "\n",
        "for l in vs_joylexicons:\n",
        "  l = l.split(',')\n",
        "  for word in l:\n",
        "    word = word.lstrip()\n",
        "    word = word.rstrip()\n",
        "    if word not in vs_joywords:\n",
        "      vs_joywords.append(word)\n",
        "\n",
        "for l in vs_sadlexicons:\n",
        "  l = l.split(',')\n",
        "  for word in l:\n",
        "    word = word.lstrip()\n",
        "    word = word.rstrip()\n",
        "    if word not in vs_sadwords:\n",
        "      vs_sadwords.append(word)\n",
        "\n",
        "#print(len(vs_joywords))\n",
        "#print(len(vs_sadwords))\n",
        "vs_document = []\n",
        "\n",
        "vs_features = []\n",
        "\n",
        "vs_joylexicons_count = dict()\n",
        "vs_sadlexicons_count = dict()"
      ],
      "metadata": {
        "id": "pxQ2JZ-y9E3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "for sent in vs_joysentences:\n",
        "  feature = []\n",
        "  x1, x2, x3 = 0, 0, 0\n",
        "\n",
        "  sent = sent_tokenize(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "    for w in words:\n",
        "      if w in vs_joywords:\n",
        "        if w not in vs_joylexicons_count:\n",
        "          vs_joylexicons_count[w] = 1\n",
        "        else:\n",
        "          vs_joylexicons_count[w] += 1\n",
        "        x1 = vs_joylexicons_count[w]\n",
        "\n",
        "      if w in vs_sadwords:\n",
        "        if w not in vs_sadlexicons_count:\n",
        "          vs_sadlexicons_count[w] = 1\n",
        "        else:\n",
        "          vs_sadlexicons_count[w] += 1\n",
        "        x2 = vs_joylexicons_count[w]\n",
        "    x3 = len(words)\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  vs_features.append(feature)\n",
        "  vs_document.append(sent)"
      ],
      "metadata": {
        "id": "LKP5VKYDYrsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8980e0-5bd1-44ee-8dac-a952dfe5d4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in vs_sadsentences:\n",
        "  feature = []\n",
        "  x1, x2, x3 = 0,0,0\n",
        "  sent = sent_tokenize(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "    for w in words:\n",
        "      if w in vs_joywords:\n",
        "        if w not in vs_joylexicons_count:\n",
        "          vs_joylexicons_count[w] = 1\n",
        "        else:\n",
        "          vs_joylexicons_count[w] += 1\n",
        "        x1 = vs_joylexicons_count[w]\n",
        "      if w in vs_sadwords:\n",
        "        if w not in vs_sadlexicons_count:\n",
        "          vs_sadlexicons_count[w] = 1\n",
        "        else:\n",
        "          vs_sadlexicons_count[w] += 1\n",
        "        x2 = vs_sadlexicons_count[w]\n",
        "    x3 = len(words)\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  vs_features.append(feature)\n",
        "  vs_document.append(sent)"
      ],
      "metadata": {
        "id": "KO02BMCmfxWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"joy lexicons count in joy sentences (validation set): \")\n",
        "print(vs_joylexicons_count)\n",
        "\n",
        "print('sad lexicons count in sad sentences (validation set): ')\n",
        "print(vs_sadlexicons_count)\n",
        "\n",
        "for f in vs_features:\n",
        "  print(f)\n"
      ],
      "metadata": {
        "id": "D8QEn3IYg1fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f870dcc-919b-4988-8d24-db3b6d183303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joy lexicons count in joy sentences (validation set): \n",
            "{'young': 3, 'daughter': 2, 'youth': 1, 'elated': 2, 'victory': 3, 'excitement': 1, 'thankful': 1, 'gift': 1, 'astonishment': 1, 'beach': 3, 'whimsical': 2, 'accomplished': 2, 'journey': 2, 'inspired': 1, 'pleasant': 1, 'blessed': 1}\n",
            "sad lexicons count in sad sentences (validation set): \n",
            "{'abandoned': 3, 'grief': 1, 'overpriced': 1, 'pandemic': 1, 'sickening': 1, 'helplessness': 1, 'betrayal': 1, 'blue': 2, 'emptiness': 2, 'invalid': 2, 'weight': 3, 'regret': 1, 'denied': 1, 'wrongful': 1, 'disappointed': 1, 'disabled': 1}\n",
            "[1, 0, 22]\n",
            "[1, 0, 21]\n",
            "[1, 0, 26]\n",
            "[2, 0, 11]\n",
            "[2, 0, 12]\n",
            "[2, 0, 25]\n",
            "[1, 0, 14]\n",
            "[0, 0, 19]\n",
            "[1, 0, 10]\n",
            "[2, 0, 18]\n",
            "[0, 1, 19]\n",
            "[0, 0, 19]\n",
            "[0, 1, 24]\n",
            "[0, 1, 7]\n",
            "[0, 1, 19]\n",
            "[0, 2, 24]\n",
            "[3, 2, 26]\n",
            "[0, 1, 17]\n",
            "[0, 1, 17]\n",
            "[0, 1, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "i = 0\n",
        "for row in vs_features:\n",
        "  row.append(classes[i])\n",
        "  i += 1\n",
        "  #print(row)\n",
        "\n",
        "print()\n",
        "# learning_rate\n",
        "#for row in vs_features:\n",
        "#  pred = prediction(row, learning_rate)\n",
        "#j = 0\n",
        "#for row in range(len(vs_features)//2):\n",
        "#  vs_features[row].append(learning_rate[j])\n",
        "#  j += 1\n",
        "#j=0\n",
        "#for row in range(len(vs_features)//2, len(vs_features)):\n",
        "#  vs_features[row].append(learning_rate[j])\n",
        "#  j += 1\n",
        "\n",
        "coef = [0,0,0]\n",
        "k = 1\n",
        "#print(vs_features)\n",
        "mn = 1.0\n",
        "r = []\n",
        "lr = 0\n",
        "for row in vs_features[:10]:\n",
        "  print(row)\n",
        "  for i in learning_rate:\n",
        "    print(i)\n",
        "    coef[0] = i\n",
        "    coef[1] = i\n",
        "    coef[2] = i\n",
        "    yhat = prediction(row, coef)\n",
        "    print(\"Expected=\", row[-1], ', Predicted=', yhat)\n",
        "    loss = Lce(yhat)\n",
        "    if loss < mn:\n",
        "      mn = loss\n",
        "      r = row\n",
        "      lr = i\n",
        "      break\n",
        "    print(\"Loss=\", loss)\n",
        "  print()\n",
        "\n",
        "print(lr)\n",
        "print('Lowest validation loss: ', mn)"
      ],
      "metadata": {
        "id": "WpVTQ9YUiHu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d61fcd-9e58-48eb-8bc9-43458d45e7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1, 0, 22, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000599999997121\n",
            "\n",
            "[1, 0, 21, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000574999997465\n",
            "Loss= 0.6930321871724454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5002874999683151\n",
            "\n",
            "[1, 0, 26, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000699999995426\n",
            "Loss= 0.6930071903599454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5003499999428334\n",
            "\n",
            "[2, 0, 11, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000349999999428\n",
            "Loss= 0.6930771830099454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5001749999928542\n",
            "Loss= 0.692797241809944\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.5003499999428334\n",
            "Loss= 0.6924474255599252\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5017499928542016\n",
            "\n",
            "[2, 0, 12, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000374999999296\n",
            "Loss= 0.6930721833724454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5001874999912109\n",
            "Loss= 0.6927722508724437\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.5003749999296875\n",
            "Loss= 0.692397461809919\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5018749912109869\n",
            "\n",
            "[2, 0, 25, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000699999995426\n",
            "Loss= 0.6930071903599454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5003499999428334\n",
            "Loss= 0.6924474255599252\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.500699999542667\n",
            "Loss= 0.6917481605596253\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5034999428344538\n",
            "\n",
            "[1, 0, 14, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000399999999147\n",
            "Loss= 0.6930671837599454\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5001999999893333\n",
            "Loss= 0.6927472605599433\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.5003999999146667\n",
            "Loss= 0.6923475005599112\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5019999893334016\n",
            "Loss= 0.689155180538612\n",
            "0.001\n",
            "Expected= 1 , Predicted= 0.5039999146688512\n",
            "\n",
            "[0, 0, 19, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.5000499999998334\n",
            "Loss= 0.6930471855599453\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5002499999791666\n",
            "Loss= 0.6926473055599401\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.5004999998333334\n",
            "Loss= 0.692147680559862\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5024999791668749\n",
            "Loss= 0.6881596805078625\n",
            "0.001\n",
            "Expected= 1 , Predicted= 0.5049998333399998\n",
            "\n",
            "[1, 0, 10, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.500029999999964\n",
            "Loss= 0.6930871823599453\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5001499999954999\n",
            "Loss= 0.6928472255599448\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.500299999964\n",
            "Loss= 0.6925473605599346\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5014999955000162\n",
            "Loss= 0.6901516805531953\n",
            "0.001\n",
            "Expected= 1 , Predicted= 0.5029999640005184\n",
            "Loss= 0.6871651804519464\n",
            "0.005\n",
            "Expected= 1 , Predicted= 0.51499550161941\n",
            "\n",
            "[2, 0, 18, 1]\n",
            "1e-05\n",
            "Expected= 1 , Predicted= 0.500052499999807\n",
            "Loss= 0.6930421860724453\n",
            "5e-05\n",
            "Expected= 1 , Predicted= 0.5002624999758828\n",
            "Loss= 0.692622318372439\n",
            "0.0001\n",
            "Expected= 1 , Predicted= 0.5005249998070626\n",
            "Loss= 0.6920977318098441\n",
            "0.0005\n",
            "Expected= 1 , Predicted= 0.5026249758830783\n",
            "Loss= 0.6879109617466383\n",
            "0.001\n",
            "Expected= 1 , Predicted= 0.5052498070710082\n",
            "Loss= 0.6827023045470532\n",
            "0.005\n",
            "Expected= 1 , Predicted= 0.5262259093720687\n",
            "\n",
            "0.005\n",
            "Lowest validation loss:  0.6420246729486955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 5- evaluating classifier on test set"
      ],
      "metadata": {
        "id": "uZZmQrW79EFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 5.1"
      ],
      "metadata": {
        "id": "Fq1UFC1d9L6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix on testing set (2x2)\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true = ['Joy', 'Sadness']\n",
        "y_pred = []\n",
        "labels = ['Joy', 'Sadness']\n",
        "\n",
        "learningRateWeights = [0.005, 0.005, 0.005]  # ?\n",
        "\n",
        "testset_joywords = []\n",
        "testset_sadwords = []\n",
        "\n",
        "testset_features = []\n",
        "testset_document = []\n",
        "\n",
        "for words in testing_set['Joy Lexicons']:\n",
        "  words = words.split(',')\n",
        "  for w in words:\n",
        "    w = w.lstrip()\n",
        "    w = w.rstrip()\n",
        "    if w not in testset_joywords:\n",
        "      testset_joywords.append(w)\n",
        "\n",
        "#print(testset_joywords)\n",
        "\n",
        "for words in testing_set['Sadness Lexicons']:\n",
        "  words = words.split(',')\n",
        "  for w in words:\n",
        "    #print(w)\n",
        "    w = w.lstrip()\n",
        "    w = w.rstrip()\n",
        "    if w not in testset_sadwords:\n",
        "      testset_sadwords.append(w)\n",
        "\n",
        "#print(testset_sadwords)\n",
        "\n",
        "ts_joylexicons = dict()\n",
        "ts_sadlexicons = dict()\n",
        "\n",
        "for sent in testing_set['Joy Sentences']:\n",
        "  feature = []\n",
        "  x1, x2, x3 = 0, 0, 0\n",
        "  sent = sent_tokenize(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "    #print(words)\n",
        "    for w in words:\n",
        "      #print(w)\n",
        "      if w in testset_joywords:\n",
        "        if w not in ts_joylexicons:\n",
        "          ts_joylexicons[w] = 1\n",
        "        else:\n",
        "          ts_joylexicons[w] += 1\n",
        "        x1 = ts_joylexicons[w]\n",
        "      if w in testset_sadwords:\n",
        "        if w not in ts_sadlexicons:\n",
        "          ts_sadlexicons[w] = 1\n",
        "        else:\n",
        "          ts_sadlexicons[w] += 1\n",
        "        x2 = ts_sadlexicons[w]\n",
        "    x3 = len(words)\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  testset_features.append(feature)\n",
        "  testset_document.append(sent)\n",
        "\n",
        "#print(testset_document)\n",
        "\n",
        "for sent in testing_set['Sadness Sentences']:\n",
        "  feature = []\n",
        "  x1, x2, x3 = 0, 0, 0\n",
        "  sent = sent_tokenize(sent)\n",
        "  for word in sent:\n",
        "    words = word_tokenize(word)\n",
        "    #print(words)\n",
        "    for w in words:\n",
        "      if w in testset_joywords:\n",
        "        #print(w)\n",
        "        if w not in ts_joylexicons:\n",
        "          ts_joylexicons[w] = 1\n",
        "        else:\n",
        "          ts_joylexicons[w] += 1\n",
        "        x1 = ts_joylexicons[w]\n",
        "\n",
        "      if w in testset_sadwords:\n",
        "        if w not in ts_sadlexicons:\n",
        "          ts_sadlexicons[w] = 1\n",
        "        else:\n",
        "          ts_sadlexicons[w] += 1\n",
        "        x2 = ts_sadlexicons[w]\n",
        "    x3 = len(words)\n",
        "  feature.append(x1)\n",
        "  feature.append(x2)\n",
        "  feature.append(x3)\n",
        "  testset_features.append(feature)\n",
        "  testset_document.append(sent)\n",
        "\n",
        "classes_test = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "i = 0\n",
        "for f in testset_features:\n",
        "  f.append(classes_test[i])\n",
        "  i += 1\n",
        "\n",
        "for f in testset_features:\n",
        "  print(f)"
      ],
      "metadata": {
        "id": "BTNZe1Hp9PQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3675c8-2dff-49de-b36a-67d70ff6f716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 18, 1]\n",
            "[1, 0, 18, 1]\n",
            "[2, 0, 21, 1]\n",
            "[0, 0, 26, 1]\n",
            "[1, 0, 23, 1]\n",
            "[1, 0, 16, 1]\n",
            "[0, 0, 20, 1]\n",
            "[0, 0, 20, 1]\n",
            "[1, 0, 11, 1]\n",
            "[0, 1, 19, 0]\n",
            "[0, 1, 15, 0]\n",
            "[0, 1, 26, 0]\n",
            "[0, 1, 15, 0]\n",
            "[0, 1, 15, 0]\n",
            "[4, 1, 10, 0]\n",
            "[0, 1, 16, 0]\n",
            "[0, 0, 16, 0]\n",
            "[0, 1, 19, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_joy = dict()\n",
        "\n",
        "y_true = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "y_pred = []\n",
        "\n",
        "for row in testset_features:\n",
        "  #print(row)\n",
        "  yhat = prediction(row, learningRateWeights)\n",
        "  #print(\"Expected=\", row[-1], \"Predicted=\", yhat)\n",
        "  if round(yhat) == 1: # joy class\n",
        "    y_pred.append(round(yhat))\n",
        "  elif round(yhat) == 0:\n",
        "    y_pred.append(round(yhat))\n",
        "  elif round(yhat) == 0:\n",
        "    y_pred.append(round(yhat))\n",
        "  elif round(yhat) == 1:\n",
        "    y_pred.append(round(yhat))\n",
        "  loss = Lce(yhat)\n",
        "  #print(\"Loss=\", loss)\n",
        "\n",
        "CM = confusion_matrix(y_true, y_pred, labels=[1,0] )\n",
        "print(CM)\n",
        "#disp = ConfusionMatrixDisplay(confusion_matrix=CM)\n",
        "#disp.plot()\n",
        "#plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP9cIgoWS1x4",
        "outputId": "83098f41-cf2d-44ac-9b2a-49e874624dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9 0]\n",
            " [9 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Section 5.2"
      ],
      "metadata": {
        "id": "lodOVpcA9PX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: ', accuracy_score(y_true, y_pred))\n",
        "print('Precision (micro): ', precision_score(y_true, y_pred, labels='Joy', average='micro'))\n",
        "print('Precision (macro): ', precision_score(y_true, y_pred, labels='Joy', average='macro'))\n",
        "print('Precision (weighted): ', precision_score(y_true, y_pred, labels='Joy', average='weighted'))\n",
        "\n",
        "print('Recall (micro): ', recall_score(y_true, y_pred, labels='Joy', average='micro'))\n",
        "print('Recall (macro): ', recall_score(y_true, y_pred, labels='Joy', average='macro'))\n",
        "print('Recall (weighted): ', recall_score(y_true, y_pred, labels='Joy', average='weighted'))\n",
        "\n",
        "print('F1-score (micro): ', f1_score(y_true, y_pred, labels='Joy', average='micro'))\n",
        "print('F1-score (macro): ', f1_score(y_true, y_pred, labels='Joy', average='macro'))\n",
        "print('F1-score (weighted): ', f1_score(y_true, y_pred, labels='Joy', average='weighted'))\n",
        "\n",
        "#target_names = ['Sadness', 'Joy']\n",
        "#print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "ehwjlOv_9Qcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f46e36f-abab-471f-eb63-49bf740aec61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.5\n",
            "Precision (micro):  0.5\n",
            "Precision (macro):  0.16666666666666666\n",
            "Precision (weighted):  0.25\n",
            "Recall (micro):  0.5\n",
            "Recall (macro):  0.3333333333333333\n",
            "Recall (weighted):  0.5\n",
            "F1-score (micro):  0.5\n",
            "F1-score (macro):  0.2222222222222222\n",
            "F1-score (weighted):  0.3333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:604: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask &= (ar1 != a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IGNORE cells below"
      ],
      "metadata": {
        "id": "-6Z71APIJCpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "wBP2ejy2UcBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b85736-5c67-42d0-8489-13de1cba98d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}